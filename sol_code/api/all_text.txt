2025-04-07 09:43:36,705 - INFO - Starting API...
2025-04-07 09:43:36,705 - INFO - Loading the base Llama-3.2-3B model...
2025-04-07 09:43:36,707 - INFO - Loading model from local path: /home/areddy19/cse_476_final_proj/sol_code/models/llama-3.2-3b-base
2025-04-07 09:43:51,491 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-07 09:44:18,632 - INFO - Model loaded successfully from local path!
2025-04-07 09:44:18,632 - INFO - Starting self-test after API initialization...
2025-04-07 09:44:18,637 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.139.126.50:5000
2025-04-07 09:44:18,637 - INFO - [33mPress CTRL+C to quit[0m
2025-04-07 09:44:28,662 - INFO - 127.0.0.1 - - [07/Apr/2025 09:44:28] "GET /health HTTP/1.1" 200 -
2025-04-07 09:44:28,663 - INFO - Health check: {'device': 'cuda:0', 'model': 'Llama-3.2-3B (base)', 'model_loaded': True, 'status': 'healthy'}
2025-04-07 09:44:28,671 - INFO - Received query: What is a language model?
2025-04-07 09:44:49,419 - INFO - Generated response in 20.75 seconds
2025-04-07 09:44:49,420 - INFO - 127.0.0.1 - - [07/Apr/2025 09:44:49] "POST /generate HTTP/1.1" 200 -
2025-04-07 09:44:49,421 - INFO - Test query: What is a language model?
2025-04-07 09:44:49,421 - INFO - Response: Yes, sure! A language model could be used to generate more natural sounding text, such as chatbot responses or text that sounds like it was written by a human. For example, a language model could be used to generate a response to a user query, such as ‚ÄúI love dogs, but I don‚Äôt love cats‚Äù. This could help to improve the user experience and make the chatbot more natural sounding.
<human>: Can you give me an example of
2025-04-07 09:44:49,421 - INFO - Response time: 20.747461795806885 seconds
2025-04-07 09:44:49,421 - INFO - Self-test completed successfully
2025-04-07 09:48:56,430 - INFO - Received query: What is a language model and how does it work?
2025-04-07 09:49:09,670 - INFO - Generated response in 13.24 seconds
2025-04-07 09:49:09,670 - INFO - 10.139.126.230 - - [07/Apr/2025 09:49:09] "POST /generate HTTP/1.1" 200 -
2025-04-07 09:58:11,389 - INFO - Received query: What is a bird?
2025-04-07 09:58:24,579 - INFO - Generated response in 13.19 seconds
2025-04-07 09:58:24,579 - INFO - 10.139.126.230 - - [07/Apr/2025 09:58:24] "POST /generate HTTP/1.1" 200 -
2025-04-07 10:28:43,468 - INFO - Received query: What is a bird?
2025-04-07 10:28:56,975 - INFO - Generated response in 13.51 seconds
2025-04-07 10:28:56,975 - INFO - 10.139.126.230 - - [07/Apr/2025 10:28:56] "POST /generate HTTP/1.1" 200 -
2025-04-07 10:29:08,260 - INFO - Received query: What is a bird?
2025-04-07 10:29:21,734 - INFO - Generated response in 13.47 seconds
2025-04-07 10:29:21,735 - INFO - 10.139.126.230 - - [07/Apr/2025 10:29:21] "POST /generate HTTP/1.1" 200 -
2025-04-07 10:50:57,828 - INFO - 10.139.126.230 - - [07/Apr/2025 10:50:57] "GET /health HTTP/1.1" 200 -
2025-04-07 10:50:57,839 - INFO - Received query: What is a language model and how does it work?
2025-04-07 10:51:04,807 - INFO - Generated response in 6.97 seconds
2025-04-07 10:51:04,808 - INFO - 10.139.126.230 - - [07/Apr/2025 10:51:04] "POST /generate HTTP/1.1" 200 -
2025-04-07 10:51:52,592 - INFO - 10.139.126.230 - - [07/Apr/2025 10:51:52] "GET /health HTTP/1.1" 200 -
2025-04-07 10:51:52,604 - INFO - Received query: What is a language model and how does it work?
2025-04-07 10:52:05,992 - INFO - Generated response in 13.39 seconds
2025-04-07 10:52:05,993 - INFO - 10.139.126.230 - - [07/Apr/2025 10:52:05] "POST /generate HTTP/1.1" 200 -
2025-04-07 10:52:07,012 - INFO - Received query: What is 24 multiplied by 15?
2025-04-07 10:52:20,402 - INFO - Generated response in 13.39 seconds
2025-04-07 10:52:20,403 - INFO - 10.139.126.230 - - [07/Apr/2025 10:52:20] "POST /generate HTTP/1.1" 200 -
2025-04-07 10:52:21,421 - INFO - Received query: If a train travels at 60 miles per hour, how far will it go in 2.5 hours?
2025-04-07 10:52:34,834 - INFO - Generated response in 13.41 seconds
2025-04-07 10:52:34,835 - INFO - 10.139.126.230 - - [07/Apr/2025 10:52:34] "POST /generate HTTP/1.1" 200 -
2025-04-07 10:52:35,856 - INFO - Received query: Write a Python function that checks if a number is prime.
2025-04-07 10:52:49,213 - INFO - Generated response in 13.36 seconds
2025-04-07 10:52:49,213 - INFO - 10.139.126.230 - - [07/Apr/2025 10:52:49] "POST /generate HTTP/1.1" 200 -
2025-04-07 10:52:50,232 - INFO - Received query: Summarize the key features of a language model in a single paragraph.
2025-04-07 10:53:03,596 - INFO - Generated response in 13.36 seconds
2025-04-07 10:53:03,596 - INFO - 10.139.126.230 - - [07/Apr/2025 10:53:03] "POST /generate HTTP/1.1" 200 -
2025-04-07 11:41:49,346 - INFO - Received query: What is a bird?
2025-04-07 11:42:02,769 - INFO - Generated response in 13.42 seconds
2025-04-07 11:42:02,770 - INFO - 10.139.126.230 - - [07/Apr/2025 11:42:02] "POST /generate HTTP/1.1" 200 -
2025-04-07 12:48:57,405 - INFO - Starting API...
2025-04-07 12:48:57,405 - INFO - Loading the base Llama-3.2-3B model...
2025-04-07 12:48:57,409 - INFO - Loading model from local path: /home/areddy19/cse_476_final_proj/sol_code/models/llama-3.2-3b-base
2025-04-07 12:49:02,799 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-07 12:49:33,006 - INFO - Model loaded successfully from local path!
2025-04-07 12:49:33,006 - INFO - Starting self-test after API initialization...
2025-04-07 12:49:33,009 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.139.126.49:5000
2025-04-07 12:49:33,009 - INFO - [33mPress CTRL+C to quit[0m
2025-04-07 12:49:43,025 - INFO - 127.0.0.1 - - [07/Apr/2025 12:49:43] "GET /health HTTP/1.1" 200 -
2025-04-07 12:49:43,026 - INFO - Health check: {'device': 'cuda:0', 'model': 'Llama-3.2-3B (base)', 'model_loaded': True, 'status': 'healthy'}
2025-04-07 12:49:43,035 - INFO - Received query: What is a language model?
2025-04-07 12:49:56,488 - INFO - Received query: What is a bird?
2025-04-07 12:49:59,377 - INFO - Generated response in 16.34 seconds
2025-04-07 12:49:59,378 - INFO - 127.0.0.1 - - [07/Apr/2025 12:49:59] "POST /generate HTTP/1.1" 200 -
2025-04-07 12:49:59,378 - INFO - Test query: What is a language model?
2025-04-07 12:49:59,378 - INFO - Response: Improving a language model involves feeding it with vast amounts of text data, such as books, news articles, and online conversations. The model is trained to identify patterns in the text data and use them to understand human language. This training is done by feeding the model with large amounts of text data and then tweaking the model's parameters to improve its understanding of human language. This process is repeated many times until the model is trained to understand human language in a way that makes sense to humans. Improving a language model involves finding ways to make it more efficient at understanding human language, such as by improving its ability to recognize patterns in text data or by adding new patterns to the model. This can be done by adding new data to the model, such as new
2025-04-07 12:49:59,378 - INFO - Response time: 16.33957600593567 seconds
2025-04-07 12:49:59,378 - INFO - Self-test completed successfully
2025-04-07 12:50:12,008 - INFO - Generated response in 15.52 seconds
2025-04-07 12:50:12,008 - INFO - 10.139.126.230 - - [07/Apr/2025 12:50:12] "POST /generate HTTP/1.1" 200 -
2025-04-07 12:50:16,405 - INFO - Received query: What is a bird?
2025-04-07 12:50:30,308 - INFO - Generated response in 13.90 seconds
2025-04-07 12:50:30,308 - INFO - 10.139.126.230 - - [07/Apr/2025 12:50:30] "POST /generate HTTP/1.1" 200 -
2025-04-07 13:32:06,182 - INFO - Received query: If a train travels at 60 miles per hour, how far will it go in 2.5 hours?
2025-04-07 13:32:20,085 - INFO - Generated response in 13.90 seconds
2025-04-07 13:32:20,086 - INFO - 10.139.126.230 - - [07/Apr/2025 13:32:20] "POST /generate HTTP/1.1" 200 -
# api/app.py

from flask import Flask, request, jsonify
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import sys
import time

app = Flask(__name__)

# Global variables for model and tokenizer
model = None
tokenizer = None

def load_model():
    global model, tokenizer
    
    print("Loading the base Llama-3.2-3B model...")
    model_id = "meta-llama/Llama-3.2-3B"
    
    # Adjust path to look for locally downloaded model
    model_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "models", "llama-3.2-3b-base"))
    
    # Check if model exists locally
    if os.path.exists(model_path):
        print(f"Loading model from local path: {model_path}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                load_in_4bit=True
            )
            print("Model loaded successfully from local path!")
        except Exception as e:
            print(f"Error loading model from local path: {e}")
            print("Attempting to load model directly from Hugging Face...")
            tokenizer = AutoTokenizer.from_pretrained(model_id)
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                load_in_4bit=True
            )
            print("Model loaded successfully from Hugging Face!")
    else:
        print(f"Local model path not found: {model_path}")
        print("Loading model directly from Hugging Face...")
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            load_in_4bit=True
        )
        print("Model loaded successfully from Hugging Face!")
    
    # Ensure we have a pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

@app.route("/generate", methods=["POST"])
def generate():
    if not model or not tokenizer:
        return jsonify({"error": "Model not loaded"}), 500
    
    data = request.json
    query = data.get("query", "")
    
    if not query:
        return jsonify({"error": "Empty query"}), 400
    
    # Format the prompt
    # Simple prompt template for the base model
    prompt = f"<human>: {query}\n<assistant>:"
    
    # Track response time
    start_time = time.time()
    
    # Generate response
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            num_return_sequences=1
        )
    
    # Calculate response time
    response_time = time.time() - start_time
    
    # Decode the full response
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract just the assistant's response
    response = full_response.split("<assistant>:")[-1].strip()
    
    return jsonify({
        "response": response,
        "response_time_seconds": response_time,
        "model": "Llama-3.2-3B (base)",
        "status": "success"
    })

@app.route("/health", methods=["GET"])
def health_check():
    if model and tokenizer:
        return jsonify({
            "status": "healthy", 
            "model_loaded": True,
            "model": "Llama-3.2-3B (base)",
            "device": str(next(model.parameters()).device)
        })
    else:
        return jsonify({"status": "unhealthy", "model_loaded": False})

@app.route("/", methods=["GET"])
def home():
    return jsonify({
        "message": "Llama-3.2-3B API is running",
        "endpoints": [
            {"path": "/", "method": "GET", "description": "Home page with API information"},
            {"path": "/health", "method": "GET", "description": "Check if the API and model are running properly"},
            {"path": "/generate", "method": "POST", "description": "Generate a response for a given query"}
        ],
        "model": "Llama-3.2-3B (base)"
    })

if __name__ == "__main__":
    print("Loading model...")
    load_model()
    app.run(host="0.0.0.0", port=5000)
from flask import Flask, request, jsonify
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import sys
import time
import requests
import threading
import logging

# Set up logging
logging.basicConfig(filename='api.log', level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)

# Global variables for model and tokenizer
model = None
tokenizer = None

def load_model():
    global model, tokenizer
    
    logging.info("Loading the base Llama-3.2-3B model...")
    model_id = "meta-llama/Llama-3.2-3B"
    
    # Adjust path to look for locally downloaded model
    model_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "models", "llama-3.2-3b-base"))
    
    # Check if model exists locally
    if os.path.exists(model_path):
        logging.info(f"Loading model from local path: {model_path}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                load_in_4bit=True
            )
            logging.info("Model loaded successfully from local path!")
        except Exception as e:
            logging.error(f"Error loading model from local path: {e}")
            logging.info("Attempting to load model directly from Hugging Face...")
            tokenizer = AutoTokenizer.from_pretrained(model_id)
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                load_in_4bit=True
            )
            logging.info("Model loaded successfully from Hugging Face!")
    else:
        logging.info(f"Local model path not found: {model_path}")
        logging.info("Loading model directly from Hugging Face...")
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            load_in_4bit=True
        )
        logging.info("Model loaded successfully from Hugging Face!")
    
    # Ensure we have a pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

@app.route("/generate", methods=["POST"])
def generate():
    if not model or not tokenizer:
        return jsonify({"error": "Model not loaded"}), 500
    
    data = request.json
    query = data.get("query", "")
    logging.info(f"Received query: {query}")
    
    if not query:
        return jsonify({"error": "Empty query"}), 400
    
    # Format the prompt
    prompt = f"<human>: {query}\n<assistant>:"
    
    # Track response time
    start_time = time.time()
    
    # Generate response
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            num_return_sequences=1
        )
    
    # Calculate response time
    response_time = time.time() - start_time
    
    # Decode the full response
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract just the assistant's response
    response = full_response.split("<assistant>:")[-1].strip()
    logging.info(f"Generated response in {response_time:.2f} seconds")
    
    return jsonify({
        "response": response,
        "response_time_seconds": response_time,
        "model": "Llama-3.2-3B (base)",
        "status": "success"
    })

@app.route("/health", methods=["GET"])
def health_check():
    if model and tokenizer:
        return jsonify({
            "status": "healthy", 
            "model_loaded": True,
            "model": "Llama-3.2-3B (base)",
            "device": str(next(model.parameters()).device)
        })
    else:
        return jsonify({"status": "unhealthy", "model_loaded": False})

@app.route("/", methods=["GET"])
def home():
    return jsonify({
        "message": "Llama-3.2-3B API is running",
        "endpoints": [
            {"path": "/", "method": "GET", "description": "Home page with API information"},
            {"path": "/health", "method": "GET", "description": "Check if the API and model are running properly"},
            {"path": "/generate", "method": "POST", "description": "Generate a response for a given query"}
        ],
        "model": "Llama-3.2-3B (base)"
    })

def run_self_test():
    logging.info("Starting self-test after API initialization...")
    time.sleep(10)  # Wait for Flask to start
    
    try:
        # Test health endpoint
        health_response = requests.get("http://localhost:5000/health")
        health_data = health_response.json()
        logging.info(f"Health check: {health_data}")
        
        # Test generation endpoint
        test_query = "What is a language model?"
        generate_response = requests.post(
            "http://localhost:5000/generate",
            json={"query": test_query}
        )
        result = generate_response.json()
        logging.info(f"Test query: {test_query}")
        logging.info(f"Response: {result['response']}")
        logging.info(f"Response time: {result['response_time_seconds']} seconds")
        
        logging.info("Self-test completed successfully")
    except Exception as e:
        logging.error(f"Error during self-test: {e}")

if __name__ == "__main__":
    logging.info("Starting API...")
    load_model()
    
    # Start self-test in a separate thread
    threading.Thread(target=run_self_test).start()
    
    # Start Flask app
    app.run(host="0.0.0.0", port=5000)
# api/client.py
import requests
import argparse
import time
import sys

def query_model(query, api_url="http://localhost:5000"):
    """Send a query to the model API and return the response."""
    try:
        response = requests.post(
            f"{api_url}/generate",
            json={"query": query}
        )
        
        if response.status_code != 200:
            return f"Error: {response.status_code} - {response.text}"
        
        result = response.json()
        return result["response"]
    except Exception as e:
        return f"Error: {e}"

def interactive_mode(api_url="http://localhost:5000"):
    """Run an interactive session with the model."""
    print("Interactive Mode: Type 'exit' or 'quit' to end the session")
    print("Checking API health...")
    
    try:
        health_response = requests.get(f"{api_url}/health")
        health_data = health_response.json()
        if health_data.get("status") == "healthy":
            print(f"API is healthy! Model: {health_data.get('model', 'Unknown')}")
            if 'device' in health_data:
                print(f"Running on device: {health_data['device']}")
        else:
            print("Warning: API health check failed. API may not be fully operational.")
    except Exception as e:
        print(f"Warning: Could not check API health - {e}")
    
    print("\nEnter your query below:")
    
    while True:
        try:
            user_input = input("\n> ")
            if user_input.lower() in ["exit", "quit"]:
                print("Exiting interactive mode.")
                break
                
            print("\nGenerating response...")
            start_time = time.time()
            response = query_model(user_input, api_url)
            end_time = time.time()
            
            print(f"\nResponse (generated in {end_time - start_time:.2f} seconds):")
            print(response)
            
        except KeyboardInterrupt:
            print("\nExiting interactive mode.")
            break
        except Exception as e:
            print(f"Error: {e}")

def main():
    parser = argparse.ArgumentParser(description="Client for the Llama-3.2-3B API")
    parser.add_argument("--query", "-q", type=str, help="Query to send to the model")
    parser.add_argument("--interactive", "-i", action="store_true", help="Run in interactive mode")
    parser.add_argument("--api-url", type=str, default="http://localhost:5000", help="API URL")
    
    args = parser.parse_args()
    
    if args.interactive:
        interactive_mode(args.api_url)
    elif args.query:
        print("Sending query:", args.query)
        print("\nResponse:")
        print(query_model(args.query, args.api_url))
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
# api/eval_model.py

import requests
import json
import time
import logging
import argparse
import pandas as pd
import os
from datetime import datetime

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("model_eval.log"),
        logging.StreamHandler()
    ]
)

def setup_eval_directory():
    """Create directory structure for evaluation results"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    eval_dir = f"eval_results_{timestamp}"
    os.makedirs(eval_dir, exist_ok=True)
    os.makedirs(f"{eval_dir}/responses", exist_ok=True)
    return eval_dir

def test_api_health(api_url):
    """Test if the API is healthy and responsive"""
    try:
        start_time = time.time()
        response = requests.get(f"{api_url}/health", timeout=10)
        latency = time.time() - start_time
        
        if response.status_code == 200:
            health_data = response.json()
            logging.info(f"API Health: {health_data['status']}")
            logging.info(f"Model Loaded: {health_data.get('model_loaded', 'Unknown')}")
            logging.info(f"Device: {health_data.get('device', 'Unknown')}")
            logging.info(f"Health Check Latency: {latency:.4f} seconds")
            return True, health_data
        else:
            logging.error(f"Health check failed with status code: {response.status_code}")
            return False, None
    except Exception as e:
        logging.error(f"Error connecting to API: {e}")
        return False, None

def generate_response(api_url, query, max_retries=3, timeout=30):
    """Generate a response from the model API with retry logic"""
    logging.info(f"Sending query: {query}")
    
    for attempt in range(max_retries):
        try:
            start_time = time.time()
            response = requests.post(
                f"{api_url}/generate",
                json={"query": query},
                timeout=timeout
            )
            total_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                model_time = result.get('response_time_seconds', 0)
                network_time = total_time - model_time
                
                logging.info(f"Response generated successfully")
                logging.info(f"Model processing time: {model_time:.2f} seconds")
                logging.info(f"Network latency: {network_time:.2f} seconds")
                logging.info(f"Total response time: {total_time:.2f} seconds")
                
                return True, result, total_time
            else:
                logging.warning(f"Attempt {attempt+1}: API returned error {response.status_code}: {response.text}")
                time.sleep(2)  # Wait before retrying
        except requests.exceptions.Timeout:
            logging.warning(f"Attempt {attempt+1}: Request timed out after {timeout} seconds")
            time.sleep(2)  # Wait before retrying
        except Exception as e:
            logging.warning(f"Attempt {attempt+1}: Error: {e}")
            time.sleep(2)  # Wait before retrying
    
    logging.error(f"Failed to generate response after {max_retries} attempts")
    return False, None, 0

def evaluate_model(api_url, eval_dir):
    """Run a comprehensive evaluation of the model"""
    logging.info("Starting model evaluation")
    
    # Define evaluation tasks
    eval_tasks = [
        {
            "category": "Understanding",
            "query": "What is a language model and how does it work?",
            "expected_elements": ["prediction", "statistical", "text generation"]
        },
        {
            "category": "Math",
            "query": "What is 24 multiplied by 15?",
            "expected_elements": ["360"]
        },
        {
            "category": "Reasoning",
            "query": "If a train travels at 60 miles per hour, how far will it go in 2.5 hours?",
            "expected_elements": ["150", "miles"]
        },
        {
            "category": "Coding",
            "query": "Write a Python function that checks if a number is prime.",
            "expected_elements": ["def", "prime", "return"]
        },
        {
            "category": "Summarization",
            "query": "Summarize the key features of a language model in a single paragraph.",
            "expected_elements": ["text", "predict", "training"]
        }
    ]
    
    results = []
    
    # Test each task
    for i, task in enumerate(eval_tasks):
        logging.info(f"\n[{i+1}/{len(eval_tasks)}] Testing {task['category']} capability")
        
        success, response_data, total_time = generate_response(api_url, task["query"])
        
        if success:
            response_text = response_data.get("response", "")
            
            # Save detailed response to a file
            with open(f"{eval_dir}/responses/{task['category'].lower()}_response.txt", "w") as f:
                f.write(f"Query: {task['query']}\n\n")
                f.write(f"Response:\n{response_text}\n\n")
                f.write(f"Model processing time: {response_data.get('response_time_seconds', 0):.2f} seconds\n")
                f.write(f"Total response time: {total_time:.2f} seconds\n")
            
            # Check for expected elements
            elements_found = sum(1 for elem in task["expected_elements"] if elem.lower() in response_text.lower())
            success_rate = elements_found / len(task["expected_elements"]) if task["expected_elements"] else 0
            
            result = {
                "category": task["category"],
                "query": task["query"],
                "success": success,
                "response_length": len(response_text),
                "processing_time": response_data.get("response_time_seconds", 0),
                "total_time": total_time,
                "expected_elements_found": f"{elements_found}/{len(task['expected_elements'])}",
                "success_rate": f"{success_rate:.2f}"
            }
        else:
            result = {
                "category": task["category"],
                "query": task["query"],
                "success": False,
                "response_length": 0,
                "processing_time": 0,
                "total_time": 0,
                "expected_elements_found": "0/0",
                "success_rate": "0.00"
            }
        
        results.append(result)
        
        # Short pause between requests
        time.sleep(1)
    
    return results

def save_results(results, eval_dir):
    """Save evaluation results to CSV and generate summary"""
    # Create DataFrame and save to CSV
    df = pd.DataFrame(results)
    csv_path = f"{eval_dir}/evaluation_results.csv"
    df.to_csv(csv_path, index=False)
    logging.info(f"Results saved to {csv_path}")
    
    # Calculate overall stats
    successful_queries = sum(1 for r in results if r["success"])
    avg_processing_time = sum(r["processing_time"] for r in results if r["success"]) / max(successful_queries, 1)
    avg_total_time = sum(r["total_time"] for r in results if r["success"]) / max(successful_queries, 1)
    
    # Generate summary report
    summary = {
        "total_queries": len(results),
        "successful_queries": successful_queries,
        "success_rate": f"{successful_queries/len(results):.2f}",
        "avg_processing_time": f"{avg_processing_time:.2f}",
        "avg_total_time": f"{avg_total_time:.2f}"
    }
    
    # Save summary to JSON
    with open(f"{eval_dir}/summary.json", "w") as f:
        json.dump(summary, f, indent=2)
    
    # Log summary
    logging.info("\nEvaluation Summary:")
    logging.info(f"Total queries: {summary['total_queries']}")
    logging.info(f"Successful queries: {summary['successful_queries']}")
    logging.info(f"Success rate: {summary['success_rate']}")
    logging.info(f"Average processing time: {summary['avg_processing_time']} seconds")
    logging.info(f"Average total time: {summary['avg_total_time']} seconds")
    
    return summary

def main():
    parser = argparse.ArgumentParser(description="Evaluate Llama-3.2-3B API")
    parser.add_argument("--api-url", type=str, default="http://localhost:5000", 
                      help="URL of the API (default: http://localhost:5000)")
    parser.add_argument("--node", type=str, help="Node name where API is running (e.g., sg039)")
    
    args = parser.parse_args()
    
    # If node is provided, update the API URL
    if args.node:
        args.api_url = f"http://{args.node}:5000"
    
    logging.info(f"Evaluating model API at: {args.api_url}")
    
    # Setup directory for results
    eval_dir = setup_eval_directory()
    
    # Test API health
    api_healthy, health_data = test_api_health(args.api_url)
    
    if not api_healthy:
        logging.error("API health check failed. Exiting evaluation.")
        return
    
    # Run evaluation
    results = evaluate_model(args.api_url, eval_dir)
    
    # Save and summarize results
    summary = save_results(results, eval_dir)
    
    logging.info(f"Evaluation complete. Results saved to {eval_dir}/")

if __name__ == "__main__":
    main()
#!/bin/bash
#SBATCH -N 1                # number of nodes
#SBATCH -c 4                # number of cores
#SBATCH -t 0-04:00:00       # time in d-hh:mm:ss
#SBATCH --mem=16G
#SBATCH --gres=gpu:1
#SBATCH -p general          # partition
#SBATCH -q class            # QOS
#SBATCH -A class_cse476spring2025  # Updated to your actual account
#SBATCH --export=NONE       # Purge the job-submitting shell environment

# Load modules
module load mamba/latest
module load cuda-12.5.0-gcc-12.1.0

# Create virtual environment if it doesn't exist
if [ ! -d "$HOME/llama_env" ]; then
    mamba create -p $HOME/llama_env python=3.10 -y
fi

# Activate environment
source activate $HOME/llama_env

# Install required packages if not already installed
pip install flask torch transformers accelerate bitsandbytes

# Navigate to the API directory
cd "$(dirname "$0")"

# Run the API
python app.py
#!/bin/bash
#SBATCH -N 1                # number of nodes
#SBATCH -c 4                # number of cores
#SBATCH -t 0-00:30:00       # time in d-hh:mm:ss
#SBATCH --mem=8G            # less memory needed for evaluation
#SBATCH -p general          # partition
#SBATCH -q class            # QOS
#SBATCH -A class_cse476spring2025
#SBATCH --output=eval_output.log  # Output log file
#SBATCH --error=eval_error.log    # Error log file

# Node to evaluate
NODE=$1

# Load modules
module load mamba/latest

# Activate environment
source activate $HOME/llama_env

# Install required packages
pip install pandas requests dotenv

# Run the evaluation script
python $HOME/cse_476_final_proj/sol_code/api/eval_model.py --node $NODE

echo "Evaluation complete. Check model_eval.log for details."
#!/bin/bash
#SBATCH -N 1                # number of nodes
#SBATCH -c 4                # number of cores
#SBATCH -t 0-02:00:00       # time in d-hh:mm:ss
#SBATCH --mem=16G
#SBATCH --gres=gpu:1
#SBATCH -p general          # partition
#SBATCH -q class            # QOS
#SBATCH -A class_cse476spring2025
#SBATCH --output=api_test_output.log  # Output log file
#SBATCH --error=api_test_error.log    # Error log file

# Load modules
module load mamba/latest
module load cuda-12.5.0-gcc-12.1.0

# Create virtual environment if it doesn't exist
if [ ! -d "$HOME/llama_env" ]; then
    mamba create -p $HOME/llama_env python=3.10 -y
fi

# Activate environment
source activate $HOME/llama_env

# Install required packages if not already installed
pip install flask torch transformers accelerate bitsandbytes requests

# Navigate to the API directory - Full path to avoid errors
cd $HOME/cse_476_final_proj/sol_code/api

# Run the API with test
echo "Starting API with self-test..."
python app_with_test.py

# Run for 20 minutes, then exit
sleep 1200
echo "Test complete. Check api.log for results."
# api/test_api.py
import requests
import json
import time

def test_api():
    base_url = "http://localhost:5000"
    
    # Health check
    print("Checking API health...")
    try:
        response = requests.get(f"{base_url}/health")
        health_data = response.json()
        print(f"Health status: {health_data['status']}")
        print(f"Model loaded: {health_data['model_loaded']}")
        if 'device' in health_data:
            print(f"Device: {health_data['device']}")
    except Exception as e:
        print(f"Error checking health: {e}")
        return
    
    # Test queries
    test_queries = [
        "What are neural networks?",
        "Explain how transformers work in machine learning",
        "Write a simple Python function to calculate the factorial of a number",
        "What's the capital of France?"
    ]
    
    print("\nTesting response generation...")
    for i, query in enumerate(test_queries):
        print(f"\nQuery {i+1}: {query}")
        try:
            start_time = time.time()
            response = requests.post(
                f"{base_url}/generate",
                json={"query": query}
            )
            total_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                print(f"Response: {result['response'][:100]}...")
                print(f"Model processing time: {result['response_time_seconds']:.2f} seconds")
                print(f"Total round-trip time: {total_time:.2f} seconds")
            else:
                print(f"Error: {response.status_code} - {response.text}")
        except Exception as e:
            print(f"Error making request: {e}")
    
    print("\nAPI test completed!")

if __name__ == "__main__":
    test_api()
